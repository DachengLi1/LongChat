{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Turotial: Coarse-grained Topic Retrieval and Fine-grained Line Retrieval Evaluation ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show the steps to conduct the Topic Retrieval and Line Retrieval evaluation on our model LongChat-13B-16K. To be specific, we present the process of evaluting the model LongChat-13B-16K on Topic Retrieval with 10-topic testcases and use our auto_topic_eval module to check the accuracy of the outputs. We demonstrate how to run the Line Retrieval evaluation with 300-line testcases as well. Through this tutorial, users will understand how to use our evaluation module and understand its output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Retrieval Evaluation ###\n",
    "In this section, we demonstrate how to run the Topic Retrieval evaluation on our model LongChat-13B-16K with 10-topic testcases and use our auto_topic_eval module to examine the accuracy of the model outputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 1: Import necessary modules ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from longeval.utils import maybe_monkey_patch, get_output_dir, longeval_load_model, load_testcases, test_topics_one_sample, test_lrt_one_sample \n",
    "from longeval.eval import longeval_test\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Configurate evaluation options and setup output directory ####\n",
    "\n",
    "Here we set the model to be evaluated as LongChat-13B-16K. The evaluation task we are running first is Topic Recall. A GPU with 40 GBs of memory is provided for this evaluation.Flash-attention is used to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output to evaluation/topics/predictions/longchat_13b_16k\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(model_name_or_path=\"lmsys/longchat_13b_16k\",\n",
    "                 task=\"topics\",\n",
    "                 num_gpus=1,\n",
    "                 max_gpu_memory=40,\n",
    "                 longchat_ratio=8,\n",
    "                 longchat_flash_attn=True)\n",
    "\n",
    "output_dir = get_output_dir(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Load patches, tokenizer, and model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmsys/longchat_13b_16k\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b510ede1f82047a5b8474f9fa487e7f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maybe_monkey_patch(args)\n",
    "model, tokenizer = longeval_load_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Start the evaluation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Start testing 10 topics per prompt ***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:19, 19.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of sustainable agriculture, Predict: ['The first topic we discussed was the future of sustainable agriculture.'], prompt length: 6472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "2it [00:39, 19.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of technology on privacy and security, Predict: ['The first topic we discussed was the impact of technology on privacy and security.'], prompt length: 6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "3it [01:02, 21.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of climate change on ocean ecosystems, Predict: ['The first topic we discussed was \"The effects of climate change on ocean ecosystems.\"'], prompt length: 6148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "4it [01:19, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of sports in society, Predict: ['The first topic we discussed was the role of sports in society.'], prompt length: 6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "5it [01:36, 18.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "6it [01:55, 18.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "7it [02:14, 19.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of technology on human connection, Predict: ['The first topic we discussed was \"The impact of technology on human connection.\"'], prompt length: 6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "8it [02:32, 18.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of reading for pleasure, Predict: ['The first topic we discussed was the benefits of reading for pleasure.'], prompt length: 6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "9it [02:48, 17.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of sports in society, Predict: ['The first topic we discussed was the role of sports in society.'], prompt length: 6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "10it [03:05, 17.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of ancient civilizations, Predict: ['The first topic we discussed was the history and culture of ancient civilizations.'], prompt length: 5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "11it [03:24, 17.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of renewable energy storage, Predict: ['The first topic we discussed was the future of renewable energy storage.'], prompt length: 6594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "12it [03:43, 18.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of spending time in nature, Predict: ['The first topic we discussed was the benefits of spending time in nature.'], prompt length: 6401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "13it [04:00, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of art in society, Predict: ['The first topic we discussed was The role of art in society.'], prompt length: 6606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "14it [04:18, 18.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "15it [04:35, 17.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of sports in society, Predict: ['The first topic we discussed was the role of sports in society.'], prompt length: 6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "16it [04:55, 18.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of the Middle Ages, Predict: ['The first topic we discussed was the history and culture of the Middle Ages.'], prompt length: 6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "17it [05:14, 18.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of mindfulness meditation, Predict: ['The first topic we discussed was the benefits of mindfulness meditation.'], prompt length: 6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "18it [05:35, 19.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of a plant-based diet, Predict: ['The first topic we discussed was the benefits of a plant-based diet.'], prompt length: 6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "19it [05:53, 18.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "20it [06:13, 19.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of a plant-based diet, Predict: ['The first topic we discussed was the benefits of a plant-based diet.'], prompt length: 6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "21it [06:28, 17.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of regular exercise, Predict: ['The first topic we discussed was the benefits of regular exercise.'], prompt length: 6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "22it [06:51, 19.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "23it [07:14, 20.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "24it [07:29, 18.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of volunteering, Predict: ['The first topic we discussed was the benefits of volunteering.'], prompt length: 6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "25it [07:49, 19.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of sustainable agriculture, Predict: ['The first topic we discussed was the future of sustainable agriculture.'], prompt length: 6424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "26it [08:09, 19.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of the Middle Ages, Predict: ['The first topic we discussed was the history and culture of the Middle Ages.'], prompt length: 6379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "27it [08:24, 18.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The psychology of happiness, Predict: ['The first topic we discussed was the psychology of happiness.'], prompt length: 6322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "28it [08:42, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of reading for pleasure, Predict: ['The first topic we discussed was \"The benefits of reading for pleasure.\"'], prompt length: 6451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "29it [09:04, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of the Middle Ages, Predict: ['The first topic we discussed was the history and culture of the Middle Ages.'], prompt length: 6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "30it [09:22, 18.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of ancient civilizations, Predict: ['The first topic we discussed was the history and culture of ancient civilizations.'], prompt length: 6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "31it [09:43, 19.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of air pollution on human health, Predict: ['The first topic we discussed was the effects of air pollution on human health.'], prompt length: 6506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "32it [09:58, 18.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of volunteering, Predict: ['The first topic we discussed was the benefits of volunteering.'], prompt length: 6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "33it [10:21, 19.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "34it [10:38, 18.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "35it [10:57, 18.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of sustainable agriculture, Predict: ['The first topic we discussed was the future of sustainable agriculture.'], prompt length: 6138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "36it [11:16, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of a plant-based diet, Predict: ['The first topic we discussed was the benefits of a plant-based diet.'], prompt length: 6156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "37it [11:35, 18.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "38it [11:51, 18.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "39it [12:09, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of technology on human connection, Predict: ['The first topic we discussed was the impact of technology on human connection.'], prompt length: 6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "40it [12:27, 17.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "41it [12:44, 17.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "42it [13:00, 17.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The psychology of happiness, Predict: ['The first topic we discussed was The psychology of happiness.'], prompt length: 6414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "43it [13:18, 17.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of sleep on overall health, Predict: ['The first topic we discussed was the effects of sleep on overall health.'], prompt length: 6310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "44it [13:40, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "45it [13:58, 18.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on communication, Predict: ['The first topic we discussed was the impact of social media on communication.'], prompt length: 6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "46it [14:14, 17.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The psychology of happiness, Predict: ['The first topic we discussed was The psychology of happiness.'], prompt length: 6583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "47it [14:30, 17.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of art in society, Predict: ['The first topic we discussed was The role of art in society.'], prompt length: 6266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "48it [14:48, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of sleep on overall health, Predict: ['The first topic we discussed was the effects of sleep on overall health.'], prompt length: 6546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "49it [15:05, 17.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of reading for pleasure, Predict: ['The first topic we discussed was \"The benefits of reading for pleasure.\"'], prompt length: 6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [15:23, 18.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6520\n",
      "************ Finish testing 10 topics per prompt with average prompt length 6380.939999999998 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we use 10-topics testcases\n",
    "num_topics = 10\n",
    "\n",
    "print(f\"************ Start testing {num_topics} topics per prompt ***********\")\n",
    "# a variable used to know the average length of the testcases\n",
    "avg_length = 0\n",
    "\n",
    "# test_file contains our pre-generated testcases\n",
    "test_file = f\"evaluation/topics/testcases/{num_topics}_topics.jsonl\"\n",
    "# the output of this evaluation is directed to output_file\n",
    "output_file = os.path.join(output_dir, f\"{num_topics}_response.txt\")\n",
    "\n",
    "# load testcases and start evaluation\n",
    "test_cases = load_testcases(test_file)\n",
    "for idx, test_case in tqdm(enumerate(test_cases)):\n",
    "    _, prompt_length, summary = test_topics_one_sample(model=model, tokenizer=tokenizer, test_case=test_case, output_file=output_file, idx=idx, args=args)\n",
    "    avg_length += prompt_length / len(test_cases)\n",
    "\n",
    "print(f\"************ Finish testing {num_topics} topics per prompt with average prompt length {avg_length} ************\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5:  Examine the model output with auto_topic_eval module ####\n",
    "Since all the model output of our evaluation is in natural language, there is not an easy method to parse and examine the correctness of these output. We developed an auto_topic_eval moduel that queries ChatGPT to assess the model outputs.\n",
    "\n",
    "Before we start using our auto_topic_eval module, we need to set OPENAI_API_KEY as an environment variable, e.g. export OPENAI_API_KEY=<YOUR_KEY>.\n",
    "\n",
    "Then we can load the model output we got from last step and pass it to ChatGPT to assess."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from longeval.auto_topic_eval import chatgpt_auto_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Start auto-evaluation, you should verify it does this correctly --------------\n",
      "Question #0: Label: The future of sustainable agriculture, model output: 'The first topic we discussed was the future of sustainable agriculture. - auto-eval goes with correct\n",
      "Question #1: Label: The impact of technology on privacy and security, model output: 'The first topic we discussed was the impact of technology on privacy and security. - auto-eval goes with correct\n",
      "Question #2: Label: The effects of climate change on ocean ecosystems, model output: 'The first topic we discussed was \"The effects of climate change on ocean ecosystems.\" - auto-eval goes with correct\n",
      "Question #3: Label: The role of sports in society, model output: 'The first topic we discussed was the role of sports in society. - auto-eval goes with correct\n",
      "Question #4: Label: The benefits of learning a new language, model output: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #5: Label: The history and impact of the Renaissance, model output: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #6: Label: The impact of technology on human connection, model output: 'The first topic we discussed was \"The impact of technology on human connection.\" - auto-eval goes with correct\n",
      "Question #7: Label: The benefits of reading for pleasure, model output: 'The first topic we discussed was the benefits of reading for pleasure. - auto-eval goes with correct\n",
      "Question #8: Label: The role of sports in society, model output: 'The first topic we discussed was the role of sports in society. - auto-eval goes with correct\n",
      "Question #9: Label: The history and culture of ancient civilizations, model output: 'The first topic we discussed was the history and culture of ancient civilizations. - auto-eval goes with correct\n",
      "Question #10: Label: The future of renewable energy storage, model output: 'The first topic we discussed was the future of renewable energy storage. - auto-eval goes with correct\n",
      "Question #11: Label: The benefits of spending time in nature, model output: 'The first topic we discussed was the benefits of spending time in nature. - auto-eval goes with correct\n",
      "Question #12: Label: The role of art in society, model output: 'The first topic we discussed was The role of art in society. - auto-eval goes with correct\n",
      "Question #13: Label: The history and impact of the Renaissance, model output: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #14: Label: The role of sports in society, model output: 'The first topic we discussed was the role of sports in society. - auto-eval goes with correct\n",
      "Question #15: Label: The history and culture of the Middle Ages, model output: 'The first topic we discussed was the history and culture of the Middle Ages. - auto-eval goes with correct\n",
      "Question #16: Label: The benefits of mindfulness meditation, model output: 'The first topic we discussed was the benefits of mindfulness meditation. - auto-eval goes with correct\n",
      "Question #17: Label: The benefits of a plant-based diet, model output: 'The first topic we discussed was the benefits of a plant-based diet. - auto-eval goes with correct\n",
      "Question #18: Label: The benefits of learning a new language, model output: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #19: Label: The benefits of a plant-based diet, model output: 'The first topic we discussed was the benefits of a plant-based diet. - auto-eval goes with correct\n",
      "Question #20: Label: The benefits of regular exercise, model output: 'The first topic we discussed was the benefits of regular exercise. - auto-eval goes with correct\n",
      "Question #21: Label: The impact of social media on mental health in adults, model output: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #22: Label: The impact of social media on mental health in adults, model output: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #23: Label: The benefits of volunteering, model output: 'The first topic we discussed was the benefits of volunteering. - auto-eval goes with correct\n",
      "Question #24: Label: The future of sustainable agriculture, model output: 'The first topic we discussed was the future of sustainable agriculture. - auto-eval goes with correct\n",
      "Question #25: Label: The history and culture of the Middle Ages, model output: 'The first topic we discussed was the history and culture of the Middle Ages. - auto-eval goes with correct\n",
      "Question #26: Label: The psychology of happiness, model output: 'The first topic we discussed was the psychology of happiness. - auto-eval goes with correct\n",
      "Question #27: Label: The benefits of reading for pleasure, model output: 'The first topic we discussed was \"The benefits of reading for pleasure.\" - auto-eval goes with correct\n",
      "Question #28: Label: The history and culture of the Middle Ages, model output: 'The first topic we discussed was the history and culture of the Middle Ages. - auto-eval goes with correct\n",
      "Question #29: Label: The history and culture of ancient civilizations, model output: 'The first topic we discussed was the history and culture of ancient civilizations. - auto-eval goes with correct\n",
      "Question #30: Label: The effects of air pollution on human health, model output: 'The first topic we discussed was the effects of air pollution on human health. - auto-eval goes with correct\n",
      "Question #31: Label: The benefits of volunteering, model output: 'The first topic we discussed was the benefits of volunteering. - auto-eval goes with correct\n",
      "Question #32: Label: The impact of social media on mental health in adults, model output: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #33: Label: The history and impact of the Renaissance, model output: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #34: Label: The future of sustainable agriculture, model output: 'The first topic we discussed was the future of sustainable agriculture. - auto-eval goes with correct\n",
      "Question #35: Label: The benefits of a plant-based diet, model output: 'The first topic we discussed was the benefits of a plant-based diet. - auto-eval goes with correct\n",
      "Question #36: Label: The benefits of learning a new language, model output: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #37: Label: The benefits of learning a new language, model output: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #38: Label: The impact of technology on human connection, model output: 'The first topic we discussed was the impact of technology on human connection. - auto-eval goes with correct\n",
      "Question #39: Label: The benefits of learning a new language, model output: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #40: Label: The history and impact of the Renaissance, model output: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #41: Label: The psychology of happiness, model output: 'The first topic we discussed was The psychology of happiness. - auto-eval goes with correct\n",
      "Question #42: Label: The effects of sleep on overall health, model output: 'The first topic we discussed was the effects of sleep on overall health. - auto-eval goes with correct\n",
      "Question #43: Label: The impact of social media on mental health in adults, model output: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #44: Label: The impact of social media on communication, model output: 'The first topic we discussed was the impact of social media on communication. - auto-eval goes with correct\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question #45: Label: The psychology of happiness, model output: 'The first topic we discussed was The psychology of happiness. - auto-eval goes with correct\n",
      "Question #46: Label: The role of art in society, model output: 'The first topic we discussed was The role of art in society. - auto-eval goes with correct\n",
      "Question #47: Label: The effects of sleep on overall health, model output: 'The first topic we discussed was the effects of sleep on overall health. - auto-eval goes with correct\n",
      "Question #48: Label: The benefits of reading for pleasure, model output: 'The first topic we discussed was \"The benefits of reading for pleasure.\" - auto-eval goes with correct\n",
      "Question #49: Label: The benefits of learning a new language, model output: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "---------- End auto-evaluation, predict accuracy 1.0 ---------------\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, 'r') as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "chatgpt_auto_eval(json_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown at the end of the outputs, the Topic Retrieval accuracy of our LongChat-13B-16K model on 10-topic testcases is 1.0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Retrieval Evaluation ###\n",
    "In this section, we show the steps to run Line Retrieval evaluation on our model LongChat-13B-16K with 300-line testcases.\n",
    "\n",
    "The steps of running Line Retrieval evaluation are very similar to those of Topic Retrieval evaluation. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import necessary modules ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from longeval.utils import maybe_monkey_patch, get_output_dir, longeval_load_model, load_testcases, test_topics_one_sample, test_lrt_one_sample \n",
    "from longeval.eval import longeval_test\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Configurate evaluation options and setup output directory ####\n",
    "\n",
    "Here we set the model to be evaluated as LongChat_13B_16K. The evaluation task we are running first is Line Retrieval. A GPU with 40 GBs of memory is provided for this evaluation.Flash-attention is used to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output to evaluation/lrt/predictions/longchat_13b_16k\n"
     ]
    }
   ],
   "source": [
    "# we change the task to \"lrt\" for Line Recall evaluation\n",
    "args = Namespace(model_name_or_path=\"lmsys/longchat_13b_16k\",\n",
    "                 task=\"lrt\",\n",
    "                 num_gpus=1,\n",
    "                 max_gpu_memory=40,\n",
    "                 longchat_ratio=8,\n",
    "                 longchat_flash_attn=True)\n",
    "\n",
    "output_dir = get_output_dir(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Load patches, tokenizer, and model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmsys/longchat_13b_16k\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n",
      "building interpolation to 16384\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dd192451995448e88d03076ba9f184fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maybe_monkey_patch(args)\n",
    "model, tokenizer = longeval_load_model(args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step4: Start the evaluation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Start testing 300 lines per LRT prompt ************\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800000; text-decoration-color: #800000\">╭─────────────────────────────── </span><span style=\"color: #800000; text-decoration-color: #800000; font-weight: bold\">Traceback </span><span style=\"color: #bf7f7f; text-decoration-color: #bf7f7f; font-weight: bold\">(most recent call last)</span><span style=\"color: #800000; text-decoration-color: #800000\"> ────────────────────────────────╮</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">&lt;module&gt;</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">16</span>                                                                                   <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">13 </span>output_file = os.path.join(output_dir, <span style=\"color: #808000; text-decoration-color: #808000\">f\"{</span>num_lines<span style=\"color: #808000; text-decoration-color: #808000\">}_response.txt\"</span>)                         <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">14 </span>                                                                                            <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">15 # load testcases and start evaluation</span>                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span>16 test_cases = load_testcases(test_file)                                                      <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">17 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">for</span> idx, test_case <span style=\"color: #ff00ff; text-decoration-color: #ff00ff\">in</span> tqdm(<span style=\"color: #00ffff; text-decoration-color: #00ffff\">enumerate</span>(test_cases)):                                          <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">18 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>correct, prompt_length, summary = test_lrt_one_sample(model=model, tokenizer=tokeniz    <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">19 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>avg_length += prompt_length / <span style=\"color: #00ffff; text-decoration-color: #00ffff\">len</span>(test_cases)                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> in <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_testcases</span>:<span style=\"color: #0000ff; text-decoration-color: #0000ff\">98</span>                                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>                                                                                                  <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 95 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">return</span> model, tokenizer                                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 96 </span>                                                                                           <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 97 </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">def</span> <span style=\"color: #00ff00; text-decoration-color: #00ff00\">load_testcases</span>(test_file):                                                             <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span> <span style=\"color: #800000; text-decoration-color: #800000\">❱ </span> 98 <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span><span style=\"color: #0000ff; text-decoration-color: #0000ff\">with</span> <span style=\"color: #00ffff; text-decoration-color: #00ffff\">open</span>(test_file, <span style=\"color: #808000; text-decoration-color: #808000\">'r'</span>) <span style=\"color: #0000ff; text-decoration-color: #0000ff\">as</span> json_file:                                                <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\"> 99 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   │   </span>json_list = <span style=\"color: #00ffff; text-decoration-color: #00ffff\">list</span>(json_file)                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">100 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>                                                                                       <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">│</span>   <span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">101 </span><span style=\"color: #7f7f7f; text-decoration-color: #7f7f7f\">│   </span>test_cases = []                                                                        <span style=\"color: #800000; text-decoration-color: #800000\">│</span>\n",
       "<span style=\"color: #800000; text-decoration-color: #800000\">╰──────────────────────────────────────────────────────────────────────────────────────────────────╯</span>\n",
       "<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-weight: bold\">OSError: </span><span style=\"font-weight: bold\">[</span>Errno <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">116</span><span style=\"font-weight: bold\">]</span> Stale file handle: <span style=\"color: #008000; text-decoration-color: #008000\">'evaluation/lrt/testcases/300_lines.jsonl'</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[31m╭─\u001b[0m\u001b[31m──────────────────────────────\u001b[0m\u001b[31m \u001b[0m\u001b[1;31mTraceback \u001b[0m\u001b[1;2;31m(most recent call last)\u001b[0m\u001b[31m \u001b[0m\u001b[31m───────────────────────────────\u001b[0m\u001b[31m─╮\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92m<module>\u001b[0m:\u001b[94m16\u001b[0m                                                                                   \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m13 \u001b[0moutput_file = os.path.join(output_dir, \u001b[33mf\u001b[0m\u001b[33m\"\u001b[0m\u001b[33m{\u001b[0mnum_lines\u001b[33m}\u001b[0m\u001b[33m_response.txt\u001b[0m\u001b[33m\"\u001b[0m)                         \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m14 \u001b[0m                                                                                            \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m15 \u001b[0m\u001b[2m# load testcases and start evaluation\u001b[0m                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m16 test_cases = load_testcases(test_file)                                                      \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m17 \u001b[0m\u001b[94mfor\u001b[0m idx, test_case \u001b[95min\u001b[0m tqdm(\u001b[96menumerate\u001b[0m(test_cases)):                                          \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m18 \u001b[0m\u001b[2m│   \u001b[0mcorrect, prompt_length, summary = test_lrt_one_sample(model=model, tokenizer=tokeniz    \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m19 \u001b[0m\u001b[2m│   \u001b[0mavg_length += prompt_length / \u001b[96mlen\u001b[0m(test_cases)                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m in \u001b[92mload_testcases\u001b[0m:\u001b[94m98\u001b[0m                                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m                                                                                                  \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 95 \u001b[0m\u001b[2m│   \u001b[0m\u001b[94mreturn\u001b[0m model, tokenizer                                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 96 \u001b[0m                                                                                           \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 97 \u001b[0m\u001b[94mdef\u001b[0m \u001b[92mload_testcases\u001b[0m(test_file):                                                             \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m \u001b[31m❱ \u001b[0m 98 \u001b[2m│   \u001b[0m\u001b[94mwith\u001b[0m \u001b[96mopen\u001b[0m(test_file, \u001b[33m'\u001b[0m\u001b[33mr\u001b[0m\u001b[33m'\u001b[0m) \u001b[94mas\u001b[0m json_file:                                                \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m 99 \u001b[0m\u001b[2m│   │   \u001b[0mjson_list = \u001b[96mlist\u001b[0m(json_file)                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m100 \u001b[0m\u001b[2m│   \u001b[0m                                                                                       \u001b[31m│\u001b[0m\n",
       "\u001b[31m│\u001b[0m   \u001b[2m101 \u001b[0m\u001b[2m│   \u001b[0mtest_cases = []                                                                        \u001b[31m│\u001b[0m\n",
       "\u001b[31m╰──────────────────────────────────────────────────────────────────────────────────────────────────╯\u001b[0m\n",
       "\u001b[1;91mOSError: \u001b[0m\u001b[1m[\u001b[0mErrno \u001b[1;36m116\u001b[0m\u001b[1m]\u001b[0m Stale file handle: \u001b[32m'evaluation/lrt/testcases/300_lines.jsonl'\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# we use 300-line testcases for this demonstration\n",
    "num_lines = 300\n",
    "\n",
    "print(f\"************ Start testing {num_lines} lines per LRT prompt ************\")\n",
    "# a variable used to know the average length of the testcases\n",
    "avg_length = 0\n",
    "# a variable used to count the number of correct model outputs\n",
    "num_correct = 0\n",
    "\n",
    "# test_file contains our pre-generated testcases\n",
    "test_file = f\"evaluation/lrt/testcases/{num_lines}_lines.jsonl\"\n",
    "# the output of this evaluation is directed to output_file\n",
    "output_file = os.path.join(output_dir, f\"{num_lines}_response.txt\")\n",
    "\n",
    "# load testcases and start evaluation\n",
    "test_cases = load_testcases(test_file)\n",
    "for idx, test_case in tqdm(enumerate(test_cases)):\n",
    "    correct, prompt_length, summary = test_lrt_one_sample(model=model, tokenizer=tokenizer, test_case=test_case, output_file=output_file, idx=idx, args=args)\n",
    "    avg_length += prompt_length / len(test_cases)\n",
    "    num_correct += correct\n",
    "accuracy = num_correct / len(test_cases)\n",
    "\n",
    "with open(output_file, \"a+\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(f\"************ Finish testing {num_lines} lines per prompt with average prompt length {avg_length}, accuracy: {accuracy} ************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
