{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Turotial: Coarse-grained Topic Retrieval and Fine-grained Line Retrieval Evaluation ###"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we show the steps to conduct the Topic Retrieval and Line Retrieval evaluation on our model LongChat-13B-16K. To be specific, we present the process of evaluating the model LongChat-13B-16K on Topic Retrieval with 10-topic testcases and use our auto_topic_eval module to check the accuracy of the outputs. We demonstrate how to run the Line Retrieval evaluation with 300-line testcases as well. Through this tutorial, users will understand how to use our evaluation moduel and understand its output."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Topic Retrieval Evaluation ###\n",
    "In this section, we demonstrate how to run the Topic Retrieval evaluation on our model LongChat-13B-16K with 10-topic testcases and use our auto_topic_eval module to examine the accuracy of the model outputs."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "#### Step 1: Import necessary modules ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from longeval.utils import maybe_monkey_patch, get_output_dir, longeval_load_model, load_testcases, test_topics_one_sample, test_lines_one_sample \n",
    "from longeval.eval import longeval_test\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Configurate evaluation options and setup output directory####\n",
    "Here we set the model to be evaluated as LongChat-13B-16K. The evaluation task we set is Topic Retrieval. A GPU with 40GBs of memory is provided for this evaluation. Flash-attention is used to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output to evaluation/topics/predictions/longchat_13b_16k\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(model_name_or_path=\"lmsys/longchat_13b_16k\",\n",
    "                 task=\"topics\",\n",
    "                 num_gpus=1,\n",
    "                 max_gpu_memory=40,\n",
    "                 longchat_ratio=8,\n",
    "                 longchat_flash_attn=True)\n",
    "\n",
    "output_dir = get_output_dir(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Load patches, tokenizer, and model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lmsys/longchat_13b_16k\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n",
      "Condensing Positional embeddings from 16384 to 2048\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34dff62b0cc34398ae75963c967facbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "maybe_monkey_patch(args)\n",
    "model, tokenizer = longeval_load_model(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Evoke the function for evaluation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Start testing 10 topics per prompt ***********\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:20, 20.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of sustainable agriculture, Predict: ['The first topic we discussed was the future of sustainable agriculture.'], prompt length: 6472\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:40, 20.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of technology on privacy and security, Predict: ['The first topic we discussed was the impact of technology on privacy and security.'], prompt length: 6479\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:04, 21.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of climate change on ocean ecosystems, Predict: ['The first topic we discussed was \"The effects of climate change on ocean ecosystems.\"'], prompt length: 6148\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [01:21, 19.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of sports in society, Predict: ['The first topic we discussed was the role of sports in society.'], prompt length: 6263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [01:38, 19.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [01:57, 19.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6690\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [02:17, 19.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of technology on human connection, Predict: ['The first topic we discussed was \"The impact of technology on human connection.\"'], prompt length: 6540\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [02:34, 18.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of reading for pleasure, Predict: ['The first topic we discussed was the benefits of reading for pleasure.'], prompt length: 6547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [02:51, 18.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of sports in society, Predict: ['The first topic we discussed was the role of sports in society.'], prompt length: 6289\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [03:09, 17.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of ancient civilizations, Predict: ['The first topic we discussed was the history and culture of ancient civilizations.'], prompt length: 5978\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [03:27, 18.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of renewable energy storage, Predict: ['The first topic we discussed was the future of renewable energy storage.'], prompt length: 6594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [03:47, 18.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of spending time in nature, Predict: ['The first topic we discussed was the benefits of spending time in nature.'], prompt length: 6401\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [04:04, 18.21s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of art in society, Predict: ['The first topic we discussed was The role of art in society.'], prompt length: 6606\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [04:23, 18.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6503\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [04:40, 17.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of sports in society, Predict: ['The first topic we discussed was the role of sports in society.'], prompt length: 6562\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [05:00, 18.67s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of the Middle Ages, Predict: ['The first topic we discussed was the history and culture of the Middle Ages.'], prompt length: 6339\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [05:19, 18.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of mindfulness meditation, Predict: ['The first topic we discussed was the benefits of mindfulness meditation.'], prompt length: 6334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [05:40, 19.43s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of a plant-based diet, Predict: ['The first topic we discussed was the benefits of a plant-based diet.'], prompt length: 6454\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [05:59, 19.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [06:19, 19.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of a plant-based diet, Predict: ['The first topic we discussed was the benefits of a plant-based diet.'], prompt length: 6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [06:34, 18.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of regular exercise, Predict: ['The first topic we discussed was the benefits of regular exercise.'], prompt length: 6197\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [06:58, 19.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [07:21, 20.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6500\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [07:37, 19.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of volunteering, Predict: ['The first topic we discussed was the benefits of volunteering.'], prompt length: 6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [07:57, 19.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of sustainable agriculture, Predict: ['The first topic we discussed was the future of sustainable agriculture.'], prompt length: 6424\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [08:17, 19.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of the Middle Ages, Predict: ['The first topic we discussed was the history and culture of the Middle Ages.'], prompt length: 6379\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [08:32, 18.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The psychology of happiness, Predict: ['The first topic we discussed was the psychology of happiness.'], prompt length: 6322\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [08:51, 18.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of reading for pleasure, Predict: ['The first topic we discussed was \"The benefits of reading for pleasure.\"'], prompt length: 6451\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [09:13, 19.41s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of the Middle Ages, Predict: ['The first topic we discussed was the history and culture of the Middle Ages.'], prompt length: 6732\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [09:32, 19.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and culture of ancient civilizations, Predict: ['The first topic we discussed was the history and culture of ancient civilizations.'], prompt length: 6162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [09:52, 19.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of air pollution on human health, Predict: ['The first topic we discussed was the effects of air pollution on human health.'], prompt length: 6506\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [10:08, 18.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of volunteering, Predict: ['The first topic we discussed was the benefits of volunteering.'], prompt length: 6336\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [10:31, 19.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6410\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [10:49, 19.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [11:08, 19.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The future of sustainable agriculture, Predict: ['The first topic we discussed was the future of sustainable agriculture.'], prompt length: 6138\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [11:28, 19.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of a plant-based diet, Predict: ['The first topic we discussed was the benefits of a plant-based diet.'], prompt length: 6156\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [11:46, 19.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6629\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [12:03, 18.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6092\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [12:21, 18.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of technology on human connection, Predict: ['The first topic we discussed was the impact of technology on human connection.'], prompt length: 6294\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [12:39, 18.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [12:57, 18.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The history and impact of the Renaissance, Predict: ['The first topic we discussed was the history and impact of the Renaissance.'], prompt length: 6338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [13:13, 17.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The psychology of happiness, Predict: ['The first topic we discussed was The psychology of happiness.'], prompt length: 6414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [13:31, 17.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of sleep on overall health, Predict: ['The first topic we discussed was the effects of sleep on overall health.'], prompt length: 6310\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [13:54, 19.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on mental health in adults, Predict: ['The first topic we discussed was the impact of social media on mental health in adults.'], prompt length: 6283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [14:12, 18.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The impact of social media on communication, Predict: ['The first topic we discussed was the impact of social media on communication.'], prompt length: 6413\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [14:28, 18.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The psychology of happiness, Predict: ['The first topic we discussed was The psychology of happiness.'], prompt length: 6583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [14:45, 17.65s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The role of art in society, Predict: ['The first topic we discussed was The role of art in society.'], prompt length: 6266\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [15:03, 17.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The effects of sleep on overall health, Predict: ['The first topic we discussed was the effects of sleep on overall health.'], prompt length: 6546\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [15:20, 17.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of reading for pleasure, Predict: ['The first topic we discussed was \"The benefits of reading for pleasure.\"'], prompt length: 6066\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [15:39, 18.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: The benefits of learning a new language, Predict: ['The first topic we discussed was the benefits of learning a new language.'], prompt length: 6520\n",
      "************ Finish testing 10 topics per prompt with average prompt length 6380.939999999998 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we use 10-topics testcases\n",
    "num_topics = 10\n",
    "\n",
    "print(f\"************ Start testing {num_topics} topics per prompt ***********\")\n",
    "# a variable used to know hte average length of the testcases\n",
    "avg_length = 0\n",
    "\n",
    "# test_file contains our pre-generated testcases\n",
    "test_file = f\"evaluation/topics/testcases/{num_topics}_topics.jsonl\"\n",
    "# the output of this evaluation is directed to output_file\n",
    "output_file = os.path.join(output_dir, f\"{num_topics}_response.txt\")\n",
    "\n",
    "# load testcases and start evaluation\n",
    "test_cases = load_testcases(test_file)\n",
    "for idx, test_case in tqdm(enumerate(test_cases)):\n",
    "    _, prompt_length, summary = test_topics_one_sample(model=model, tokenizer=tokenizer, test_case=test_case, output_file=output_file, idx=idx, args=args)\n",
    "    avg_length += prompt_length / len(test_cases)\n",
    "\n",
    "print(f\"************ Finish testing {num_topics} topics per prompt with average prompt length {avg_length} ************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 5: Examine the model output with auto_topic_eval module ####\n",
    "Since all the model output of the Topic Retrieval evaluation is in natural language, there is not an easy method to parse and examine the correctness of these output. We developed an auto_topic_eval module that queries ChatGPT to assess the model outputs. \n",
    "\n",
    "Before we start using our auto_topic_eval module, we need to set OPENAI_API_KEY as an environment variable, e.g. export OPENAI_API_KEY=<YOUR_KEY>.\n",
    "\n",
    "Then we can import the module, load the model output we got from last step and pass it to ChatGPT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from longeval.auto_topic_eval import chatgpt_auto_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- Start auto-evaluation, you should verify it does this correctly --------------\n",
      "Question #0: Label: The future of sustainable agriculture, Predict: 'The first topic we discussed was the future of sustainable agriculture. - auto-eval goes with correct\n",
      "Question #1: Label: The impact of technology on privacy and security, Predict: 'The first topic we discussed was the impact of technology on privacy and security. - auto-eval goes with correct\n",
      "Question #2: Label: The effects of climate change on ocean ecosystems, Predict: 'The first topic we discussed was \"The effects of climate change on ocean ecosystems.\" - auto-eval goes with correct\n",
      "Question #3: Label: The role of sports in society, Predict: 'The first topic we discussed was the role of sports in society. - auto-eval goes with correct\n",
      "Question #4: Label: The benefits of learning a new language, Predict: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #5: Label: The history and impact of the Renaissance, Predict: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #6: Label: The impact of technology on human connection, Predict: 'The first topic we discussed was \"The impact of technology on human connection.\" - auto-eval goes with correct\n",
      "Question #7: Label: The benefits of reading for pleasure, Predict: 'The first topic we discussed was the benefits of reading for pleasure. - auto-eval goes with correct\n",
      "Question #8: Label: The role of sports in society, Predict: 'The first topic we discussed was the role of sports in society. - auto-eval goes with correct\n",
      "Question #9: Label: The history and culture of ancient civilizations, Predict: 'The first topic we discussed was the history and culture of ancient civilizations. - auto-eval goes with correct\n",
      "Question #10: Label: The future of renewable energy storage, Predict: 'The first topic we discussed was the future of renewable energy storage. - auto-eval goes with correct\n",
      "Question #11: Label: The benefits of spending time in nature, Predict: 'The first topic we discussed was the benefits of spending time in nature. - auto-eval goes with correct\n",
      "Question #12: Label: The role of art in society, Predict: 'The first topic we discussed was The role of art in society. - auto-eval goes with correct\n",
      "Question #13: Label: The history and impact of the Renaissance, Predict: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #14: Label: The role of sports in society, Predict: 'The first topic we discussed was the role of sports in society. - auto-eval goes with correct\n",
      "Question #15: Label: The history and culture of the Middle Ages, Predict: 'The first topic we discussed was the history and culture of the Middle Ages. - auto-eval goes with correct\n",
      "Question #16: Label: The benefits of mindfulness meditation, Predict: 'The first topic we discussed was the benefits of mindfulness meditation. - auto-eval goes with correct\n",
      "Question #17: Label: The benefits of a plant-based diet, Predict: 'The first topic we discussed was the benefits of a plant-based diet. - auto-eval goes with correct\n",
      "Question #18: Label: The benefits of learning a new language, Predict: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #19: Label: The benefits of a plant-based diet, Predict: 'The first topic we discussed was the benefits of a plant-based diet. - auto-eval goes with correct\n",
      "Question #20: Label: The benefits of regular exercise, Predict: 'The first topic we discussed was the benefits of regular exercise. - auto-eval goes with correct\n",
      "Question #21: Label: The impact of social media on mental health in adults, Predict: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #22: Label: The impact of social media on mental health in adults, Predict: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #23: Label: The benefits of volunteering, Predict: 'The first topic we discussed was the benefits of volunteering. - auto-eval goes with correct\n",
      "Question #24: Label: The future of sustainable agriculture, Predict: 'The first topic we discussed was the future of sustainable agriculture. - auto-eval goes with correct\n",
      "Question #25: Label: The history and culture of the Middle Ages, Predict: 'The first topic we discussed was the history and culture of the Middle Ages. - auto-eval goes with correct\n",
      "Question #26: Label: The psychology of happiness, Predict: 'The first topic we discussed was the psychology of happiness. - auto-eval goes with correct\n",
      "Question #27: Label: The benefits of reading for pleasure, Predict: 'The first topic we discussed was \"The benefits of reading for pleasure.\" - auto-eval goes with correct\n",
      "Question #28: Label: The history and culture of the Middle Ages, Predict: 'The first topic we discussed was the history and culture of the Middle Ages. - auto-eval goes with correct\n",
      "Question #29: Label: The history and culture of ancient civilizations, Predict: 'The first topic we discussed was the history and culture of ancient civilizations. - auto-eval goes with correct\n",
      "Question #30: Label: The effects of air pollution on human health, Predict: 'The first topic we discussed was the effects of air pollution on human health. - auto-eval goes with correct\n",
      "Question #31: Label: The benefits of volunteering, Predict: 'The first topic we discussed was the benefits of volunteering. - auto-eval goes with correct\n",
      "Question #32: Label: The impact of social media on mental health in adults, Predict: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #33: Label: The history and impact of the Renaissance, Predict: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #34: Label: The future of sustainable agriculture, Predict: 'The first topic we discussed was the future of sustainable agriculture. - auto-eval goes with correct\n",
      "Question #35: Label: The benefits of a plant-based diet, Predict: 'The first topic we discussed was the benefits of a plant-based diet. - auto-eval goes with correct\n",
      "Question #36: Label: The benefits of learning a new language, Predict: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #37: Label: The benefits of learning a new language, Predict: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #38: Label: The impact of technology on human connection, Predict: 'The first topic we discussed was the impact of technology on human connection. - auto-eval goes with correct\n",
      "Question #39: Label: The benefits of learning a new language, Predict: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "Question #40: Label: The history and impact of the Renaissance, Predict: 'The first topic we discussed was the history and impact of the Renaissance. - auto-eval goes with correct\n",
      "Question #41: Label: The psychology of happiness, Predict: 'The first topic we discussed was The psychology of happiness. - auto-eval goes with correct\n",
      "Question #42: Label: The effects of sleep on overall health, Predict: 'The first topic we discussed was the effects of sleep on overall health. - auto-eval goes with correct\n",
      "Question #43: Label: The impact of social media on mental health in adults, Predict: 'The first topic we discussed was the impact of social media on mental health in adults. - auto-eval goes with correct\n",
      "Question #44: Label: The impact of social media on communication, Predict: 'The first topic we discussed was the impact of social media on communication. - auto-eval goes with correct\n",
      "Question #45: Label: The psychology of happiness, Predict: 'The first topic we discussed was The psychology of happiness. - auto-eval goes with correct\n",
      "Question #46: Label: The role of art in society, Predict: 'The first topic we discussed was The role of art in society. - auto-eval goes with correct\n",
      "Question #47: Label: The effects of sleep on overall health, Predict: 'The first topic we discussed was the effects of sleep on overall health. - auto-eval goes with correct\n",
      "Question #48: Label: The benefits of reading for pleasure, Predict: 'The first topic we discussed was \"The benefits of reading for pleasure.\" - auto-eval goes with correct\n",
      "Question #49: Label: The benefits of learning a new language, Predict: 'The first topic we discussed was the benefits of learning a new language. - auto-eval goes with correct\n",
      "---------- End auto-evaluation, predict accuracy 1.0 ---------------\n"
     ]
    }
   ],
   "source": [
    "with open(output_file, \"r\") as json_file:\n",
    "    json_list = list(json_file)\n",
    "\n",
    "chatgpt_auto_eval(json_list)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown at the end of the above outputs, the Topic Retrieval accuracy of our LongChat-13B-16K model on 10-topic testcases is 1.0."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Line Retrieval Evaluation ###\n",
    "In this section, we show the steps to run Line Retrieval evaluation on our model LongChat-13B-16K with 300-line testcases.\n",
    "\n",
    "The steps of running Line Retrieval evaluation are very similar to those of Topic Retrieval evaluation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Import necessary modules ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from longeval.utils import maybe_monkey_patch, get_output_dir, longeval_load_model, load_testcases, test_topics_one_sample, test_lines_one_sample \n",
    "from longeval.eval import longeval_test\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "\n",
    "import os"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Configurate evaluation options and setup output directory####\n",
    "Here we set the model to be evaluated as LongChat-13B-16K. The evaluation task we set is Line Retrieval. A GPU with 40GBs of memory is provided for this evaluation. Flash-attention is used to save memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output to evaluation/lrt/predictions/longchat_13b_16k\n"
     ]
    }
   ],
   "source": [
    "args = Namespace(model_name_or_path=\"lmsys/longchat_13b_16k\",\n",
    "                 task=\"lrt\",\n",
    "                 num_gpus=1,\n",
    "                 max_gpu_memory=40,\n",
    "                 longchat_ratio=8,\n",
    "                 longchat_flash_attn=True)\n",
    "\n",
    "output_dir = get_output_dir(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Load patches, tokenizer, and model ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maybe_monkey_patch(args)\n",
    "model, tokenizer = longeval_load_model(args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4: Start the evaluation ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************ Start testing 300 lines per LRT prompt ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [00:35, 35.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 29324, Predict: The <REGISTER_CONTENT> in line racial-fedora is <29324>., Parsed: 29324, prompt length: 7117\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [01:11, 35.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 29602, Predict: The <REGISTER_CONTENT> in line scandalous-typewriter is <29602>., Parsed: 29602, prompt length: 7136\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [01:46, 35.66s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 34055, Predict: The <REGISTER_CONTENT> in line measly-pocketbook is <34055>., Parsed: 34055, prompt length: 7126\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [02:21, 35.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 41534, Predict: The <REGISTER_CONTENT> in line agreeable-cleric is <41534>., Parsed: 41534, prompt length: 7123\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [02:56, 35.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 34230, Predict: The <REGISTER_CONTENT> in line internal-adulthood is <34230>., Parsed: 34230, prompt length: 7116\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [03:31, 34.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 28113, Predict: The <REGISTER_CONTENT> in line erratic-dinner is <28113>., Parsed: 28113, prompt length: 7086\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [04:05, 34.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 41644, Predict: The <REGISTER_CONTENT> in line dirty-briefly is <41644>., Parsed: 41644, prompt length: 7108\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [04:39, 34.61s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 15711, Predict: The <REGISTER_CONTENT> in line worried-monitor is <15711>., Parsed: 15711, prompt length: 7142\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [05:15, 34.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 18766, Predict: The <REGISTER_CONTENT> in line screeching-testing is <18766>., Parsed: 18766, prompt length: 7086\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [05:49, 34.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 13860, Predict: The <REGISTER_CONTENT> in line decorous-afterlife is <13860>., Parsed: 13860, prompt length: 7129\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [06:28, 35.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 14266, Predict: The <REGISTER_CONTENT> in line highfalutin-arrogance is <14266>., Parsed: 14266, prompt length: 7139\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [07:02, 35.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 28264, Predict: The <REGISTER_CONTENT> in line innocent-pony is <28264>., Parsed: 28264, prompt length: 7104\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [07:35, 34.68s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1470, Predict: The <REGISTER_CONTENT> in line hot-chairperson is <1470>., Parsed: 1470, prompt length: 7147\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [08:08, 34.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1217, Predict: The <REGISTER_CONTENT> in line noisy-ecology is <1217>., Parsed: 1217, prompt length: 7133\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [08:45, 35.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 21374, Predict: The <REGISTER_CONTENT> in line relieved-guacamole is <21374>., Parsed: 21374, prompt length: 7142\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [09:19, 34.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5195, Predict: The <REGISTER_CONTENT> in line momentous-ruckus is <34408>., Parsed: 34408, prompt length: 7125\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [09:54, 34.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 42378, Predict: The <REGISTER_CONTENT> in line hissing-architect is <42378>., Parsed: 42378, prompt length: 7144\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [10:30, 35.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 39807, Predict: The <REGISTER_CONTENT> in line apathetic-iris is <39807>., Parsed: 39807, prompt length: 7151\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [11:05, 35.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 38877, Predict: The <REGISTER_CONTENT> in line talented-military is <38877>., Parsed: 38877, prompt length: 7114\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [11:40, 34.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 11672, Predict: The <REGISTER_CONTENT> in line foolish-karate is <11672>., Parsed: 11672, prompt length: 7123\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [12:14, 34.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 800, Predict: The <REGISTER_CONTENT> in line wet-leptocephalus is <800>., Parsed: 800, prompt length: 7110\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [12:51, 35.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2547, Predict: The <REGISTER_CONTENT> in line berserk-cappelletti is <2547>., Parsed: 2547, prompt length: 7140\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [13:27, 35.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 41865, Predict: The <REGISTER_CONTENT> in line wacky-cob is <41865>., Parsed: 41865, prompt length: 7101\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [14:01, 35.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 3957, Predict: The <REGISTER_CONTENT> in line obedient-pony is <3957>., Parsed: 3957, prompt length: 7090\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [14:37, 35.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 47089, Predict: The <REGISTER_CONTENT> in line open-hippodrome is <47089>., Parsed: 47089, prompt length: 7158\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [15:11, 35.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 20619, Predict: The <REGISTER_CONTENT> in line dull-trap is <20619>., Parsed: 20619, prompt length: 7107\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [15:44, 34.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 38707, Predict: The <REGISTER_CONTENT> in line young-hair is <38707>., Parsed: 38707, prompt length: 7112\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [16:18, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 48262, Predict: The <REGISTER_CONTENT> in line capable-ripple is <48262>., Parsed: 48262, prompt length: 7132\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [16:53, 34.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 14845, Predict: The <REGISTER_CONTENT> in line ignorant-purple is <14845>., Parsed: 14845, prompt length: 7143\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [17:24, 33.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 39683, Predict: The <REGISTER_CONTENT> in line ugly-real is <39683>., Parsed: 39683, prompt length: 7139\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [18:00, 34.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9156, Predict: The <REGISTER_CONTENT> in line abashed-dulcimer is <9156>., Parsed: 9156, prompt length: 7108\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [18:34, 34.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 41880, Predict: The <REGISTER_CONTENT> in line sour-sensor is <41880>., Parsed: 41880, prompt length: 7149\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [19:11, 35.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9659, Predict: The <REGISTER_CONTENT> in line crabby-lilac is <10923>., Parsed: 10923, prompt length: 7138\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [19:46, 34.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 8559, Predict: The <REGISTER_CONTENT> in line macho-cymbal is <8559>., Parsed: 8559, prompt length: 7160\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [20:19, 34.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 24384, Predict: The <REGISTER_CONTENT> in line industrious-discussion is <24384>., Parsed: 24384, prompt length: 7141\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [20:53, 34.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 18869, Predict: The <REGISTER_CONTENT> in line unable-rudiment is <18869>., Parsed: 18869, prompt length: 7136\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [21:26, 33.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 42911, Predict: The <REGISTER_CONTENT> in line calm-lunge is <42911>., Parsed: 42911, prompt length: 7134\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [22:03, 34.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 47394, Predict: The <REGISTER_CONTENT> in line tranquil-detective is <47394>., Parsed: 47394, prompt length: 7123\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [22:39, 35.07s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 5917, Predict: The <REGISTER_CONTENT> in line flippant-iron is <5917>., Parsed: 5917, prompt length: 7113\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [23:15, 35.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 45363, Predict: The <REGISTER_CONTENT> in line flagrant-championship is <45363>., Parsed: 45363, prompt length: 7135\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [23:50, 35.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 2391, Predict: The <REGISTER_CONTENT> in line tacit-banyan is <2391>., Parsed: 2391, prompt length: 7138\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [24:25, 35.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6694, Predict: The <REGISTER_CONTENT> in line lamentable-clarification is <6694>., Parsed: 6694, prompt length: 7153\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [24:56, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 178, Predict: The <REGISTER_CONTENT> in line impossible-mattress is <178>., Parsed: 178, prompt length: 7117\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [25:31, 34.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 40530, Predict: The <REGISTER_CONTENT> in line low-struggle is <40530>., Parsed: 40530, prompt length: 7119\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "45it [26:02, 33.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 9638, Predict: The <REGISTER_CONTENT> in line mighty-hemp is <9638>., Parsed: 9638, prompt length: 7116\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "46it [26:37, 33.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 30419, Predict: The <REGISTER_CONTENT> in line spicy-indicator is <30419>., Parsed: 30419, prompt length: 7156\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "47it [27:12, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 27954, Predict: The <REGISTER_CONTENT> in line bewildered-robe is <27954>., Parsed: 27954, prompt length: 7111\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "48it [27:47, 34.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 1639, Predict: The <REGISTER_CONTENT> in line itchy-satisfaction is <1639>., Parsed: 1639, prompt length: 7135\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "49it [28:21, 34.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 41767, Predict: The <REGISTER_CONTENT> in line bewildered-craft is <17565>., Parsed: 17565, prompt length: 7110\n",
      "Using conversation template: vicuna_v1.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "50it [28:55, 34.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label: 6138, Predict: The <REGISTER_CONTENT> in line faulty-sushi is <6138>., Parsed: 6138, prompt length: 7119\n",
      "************ Finish testing 300 lines per prompt with average prompt length 7126.68, accuracy: 0.94 ************\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# we use 300-line testcases for this demonstration\n",
    "num_lines = 300\n",
    "\n",
    "print(f\"************ Start testing {num_lines} lines per LRT prompt ************\")\n",
    "# a variable used to know the average length of the testcases\n",
    "avg_length = 0\n",
    "# a variable used to count the number of correct model outputs\n",
    "num_correct = 0\n",
    "\n",
    "# test_file contains our pre-generated testcases\n",
    "test_file = f\"evaluation/lines/testcases/{num_lines}_lines.jsonl\"\n",
    "# the output of this evaluation is directed to output_file\n",
    "output_file = os.path.join(output_dir, f\"{num_lines}_response.txt\")\n",
    "\n",
    "# load testcases and start evaluation\n",
    "test_cases = load_testcases(test_file)\n",
    "for idx, test_case in tqdm(enumerate(test_cases)):\n",
    "    correct, prompt_length, summary = test_lines_one_sample(model=model, tokenizer=tokenizer, test_case=test_case, output_file=output_file, idx=idx, args=args)\n",
    "    avg_length += prompt_length / len(test_cases)\n",
    "    num_correct += correct\n",
    "accuracy = num_correct / len(test_cases)\n",
    "\n",
    "with open(output_file, \"a+\") as f:\n",
    "    f.write(f\"Accuracy: {accuracy}\")\n",
    "\n",
    "print(f\"************ Finish testing {num_lines} lines per prompt with average prompt length {avg_length}, accuracy: {accuracy} ************\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the end of the output, our model has an accuracy of 0.94 on 300-line Line Retrieval testcases."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
