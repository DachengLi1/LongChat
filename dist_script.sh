#!/bin/bash
python -m torch.distributed.run --nproc_per_node=8 --nnodes 2 --node_rank $1 --master_addr dgx-01 --master_port 6381 longchat/train/fine_tune/train_interpolate.py   --model_name_or_path /data/dacheng/llama-7B-hf --data_path ~/dacheng/sharegpt_20230515_clean_lang_gpt4_32K.json  --bf16  --output_dir /data/dacheng/vicuna_ft_32K_scale  --num_train_epochs 3     --per_device_train_batch_size 1     --per_device_eval_batch_size 4     --gradient_accumulation_steps 1     --evaluation_strategy "no"     --save_strategy "steps"     --save_steps 1000     --save_total_limit 1     --learning_rate 2e-5     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type "cosine"     --logging_steps 1     --fsdp "full_shard auto_wrap"     --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer'     --tf32 True     --model_max_length 32768    --gradient_checkpointing True     --lazy_preprocess True 

