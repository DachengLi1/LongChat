python -m torch.distributed.run --nproc_per_node=8  longchat/train/fine_tune/train_condense_16K.py   --model_name_or_path /data/dacheng/llama-13B-hf --data_path data/dummy_conversation.json  --bf16  --output_dir outputs --num_train_epochs 3     --per_device_train_batch_size 1     --per_device_eval_batch_size 4     --gradient_accumulation_steps 1     --evaluation_strategy "no"     --save_strategy "steps"     --save_steps 1000     --save_total_limit 1     --learning_rate 2e-5     --weight_decay 0.     --warmup_ratio 0.03     --lr_scheduler_type "cosine"     --logging_steps 1     --fsdp "full_shard auto_wrap"     --fsdp_transformer_layer_cls_to_wrap 'LlamaDecoderLayer'     --tf32 True     --model_max_length 16384    --gradient_checkpointing True     --lazy_preprocess True 

